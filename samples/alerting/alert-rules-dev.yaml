# Prometheus alert rules for Weather Alert Service - Development Environment
# Aligned with docs/alerting-thresholds.md
#
# Development environment thresholds are more lenient to reduce alert fatigue
# while still catching critical issues. See docs/alerting-thresholds.md for
# threshold rationale and tuning guidance.
#
# Metric sources:
# - Application: httpRequestsTotal, httpRequestDurationSeconds, httpRequestsInFlight,
#   weatherApiCallsTotal, weatherApiDurationSeconds, weatherApiRetriesTotal,
#   cacheHitsTotal, weatherQueriesTotal, weatherQueriesByLocationTotal,
#   rateLimitRequestsInWindow, rateLimitRejectsInWindow, rateLimitDeniedTotal
# - Runtime: process_* and go_* (Prometheus process/go collectors)

groups:
  - name: weather-service-availability
    rules:
      # Instance down: target not scrapeable (service crashed, network, etc.)
      # Dev: 2 minutes (more lenient than prod 1 minute)
      - alert: WeatherServiceDown
        expr: up{job="weather-alert-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Weather service instance is down"
          description: "Instance {{ $labels.instance }} of job {{ $labels.job }} has been unreachable for more than 2 minutes."
          runbook_url: "https://example.com/runbooks/weather-service-down"

  - name: weather-service-request-layer
    rules:
      # High 5xx error rate: > 10% (dev: more lenient than prod 1%)
      - alert: HighHTTPErrorRate
        expr: |
          sum(rate(httpRequestsTotal{job="weather-alert-service", statusCode=~"5.."}[5m]))
          /
          sum(rate(httpRequestsTotal{job="weather-alert-service"}[5m]))
          > 0.10
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "High HTTP 5xx error rate"
          description: "More than 10% of requests are returning 5xx ({{ $value | humanizePercentage }}) over the last 10 minutes."
          runbook_url: "https://example.com/runbooks/high-error-rate"

      # High request latency: p95 > 10s (dev: more lenient than prod 2s)
      - alert: HighHTTPLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(httpRequestDurationSeconds_bucket{job="weather-alert-service"}[5m])) by (le, route)
          ) > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High HTTP request latency (p95 > 10s)"
          description: "Route {{ $labels.route }} p95 latency is {{ $value | humanize }}s."
          runbook_url: "https://example.com/runbooks/high-latency"

      # Saturation: in-flight requests consistently high
      # Dev: > 100 (more lenient than prod 50)
      - alert: HighRequestSaturation
        expr: httpRequestsInFlight{job="weather-alert-service"} > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High request saturation"
          description: "{{ $value }} requests in flight; may indicate capacity limits or slow downstream."

      # Rate limit rejections: actively rejecting requests (429s in window)
      # Dev: > 20 (more lenient than prod 10)
      - alert: HighRateLimitRejections
        expr: rateLimitRejectsInWindow{job="weather-alert-service"} > 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High rate limit rejections"
          description: "{{ $value }} requests denied (429) in sliding window; service is at capacity."
          runbook_url: "https://example.com/runbooks/rate-limit-rejections"

      # Rate limit requests approaching capacity: high request volume in window
      # Dev: Higher threshold (tune based on dev rate_limit_rps * lifecycle_window)
      # Example: if rate_limit_rps=200 and lifecycle_window=60s, threshold might be 9600 (80% of 12000)
      - alert: RateLimitRequestsHigh
        expr: rateLimitRequestsInWindow{job="weather-alert-service"} > 9600
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Rate limit requests approaching capacity"
          description: "{{ $value }} requests in window; approaching rate limit capacity. Tune threshold based on rate_limit_rps * lifecycle_window config."
          runbook_url: "https://example.com/runbooks/rate-limit-capacity"

      # High 4xx error rate: client errors (bad requests, not found, etc.)
      # Dev: > 15% (more lenient than prod 5%)
      - alert: HighHTTP4xxErrorRate
        expr: |
          sum(rate(httpRequestsTotal{job="weather-alert-service", statusCode=~"4.."}[5m]))
          /
          sum(rate(httpRequestsTotal{job="weather-alert-service"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High HTTP 4xx error rate"
          description: "More than 15% of requests are returning 4xx client errors ({{ $value | humanizePercentage }}) over the last 10 minutes."
          runbook_url: "https://example.com/runbooks/high-4xx-error-rate"

      # Zero requests: service receiving no traffic unexpectedly
      # Dev: 15 minutes (more lenient than prod 10 minutes)
      - alert: ZeroHTTPRequests
        expr: |
          sum(rate(httpRequestsTotal{job="weather-alert-service"}[15m])) == 0
          and
          sum(rate(httpRequestsTotal{job="weather-alert-service"}[30m])) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Zero HTTP requests received"
          description: "Service received no requests in the last 15 minutes but had traffic in the previous 30 minutes. Possible routing issue or service isolation."
          runbook_url: "https://example.com/runbooks/zero-requests"

  - name: weather-service-upstream
    rules:
      # Weather API error ratio: high failure rate from OpenWeatherMap
      # Dev: > 30% (more lenient than prod 10%)
      - alert: WeatherAPIHighErrorRate
        expr: |
          sum(rate(weatherApiCallsTotal{job="weather-alert-service", status=~"error|server_error|rate_limited"}[5m]))
          /
          sum(rate(weatherApiCallsTotal{job="weather-alert-service"}[5m]))
          > 0.30
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Weather API high error rate"
          description: "More than 30% of OpenWeatherMap calls are failing ({{ $value | humanizePercentage }})."
          runbook_url: "https://example.com/runbooks/upstream-api-errors"

      # Weather API latency: p95 > 5s (dev: more lenient than prod 2s)
      - alert: WeatherAPISlow
        expr: |
          histogram_quantile(0.95,
            sum(rate(weatherApiDurationSeconds_bucket{job="weather-alert-service", status="success"}[5m])) by (le)
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Weather API slow (p95 > 5s)"
          description: "OpenWeatherMap API p95 latency is {{ $value | humanize }}s."
          runbook_url: "https://example.com/runbooks/upstream-slow"

      # High retries: unstable upstream (transient failures causing retries)
      # Dev: > 2 retries/s (more lenient than prod 1 retry/s)
      - alert: WeatherAPIHighRetries
        expr: |
          rate(weatherApiRetriesTotal{job="weather-alert-service"}[5m]) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High weather API retry rate"
          description: "Retry rate {{ $value | humanize }}/s indicates unstable upstream."

  - name: weather-service-cache
    rules:
      # Low cache hit rate: cache effectiveness degraded
      # Dev: < 30% (more lenient than prod 50%)
      - alert: LowCacheHitRate
        expr: |
          sum(rate(cacheHitsTotal{job="weather-alert-service"}[5m]))
          /
          sum(rate(weatherQueriesTotal{job="weather-alert-service"}[5m]))
          < 0.30
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Low cache hit rate (< 30%)"
          description: "Cache hit rate is {{ $value | humanizePercentage }}; cache effectiveness degraded. May indicate TTL too short, cache eviction, or cache backend issues."
          runbook_url: "https://example.com/runbooks/low-cache-hit-rate"

  - name: weather-service-runtime
    rules:
      # Memory growth: possible leak (RSS increasing)
      # Dev: > 1GB (more lenient than prod 500MB)
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes{job="weather-alert-service"} > 1000000000
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High process memory (RSS > 1GB)"
          description: "RSS is {{ $value | humanize }}B."
          runbook_url: "https://example.com/runbooks/memory"

      # Goroutine leak: sudden or sustained high count
      # Dev: > 1000 (more lenient than prod 500)
      - alert: HighGoroutineCount
        expr: go_goroutines{job="weather-alert-service"} > 1000
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High goroutine count"
          description: "{{ $value }} goroutines; possible leak."
          runbook_url: "https://example.com/runbooks/goroutines"
